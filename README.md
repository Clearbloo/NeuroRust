# NeuroRust

TODO
---
- [ ] Add weight update from gradient using learning rate
- [ ] Add opitmizers (ADAM and SGD)
- [ ] Add a utils
- [x] Add tests
- [ ] Naming consistency between Loss and Activation
